Outline
待测：
1. 视觉观测时怎么样的？
2. 相似度如何？
3. 变换后如何？
4. 确认好是kv cache还是token cache？
    Outline的写法：第一章，每个自然段浓缩成一句话，简要概括本段要写什么。第二章开始，先确定每个小标题，内容可以暂时空着。

## Introduction

**Para1 VLN+计算瓶颈+加速概览→收窄到token caching**

VLN模型赋予具身智能体强大的导航泛化能力，但其高计算开销与实时导航需求之间存在矛盾。为加速VLN推理，现有方法主要沿token剪枝、记忆压缩、架构改进三条路线展开，但共同局限在于：要么有损，要么侵入式，难以即插即用。相比之下，token caching是一类免训练、即插即用的加速思路，通过跨帧复用静态视觉token的计算结果来消除时间冗余，展现了良好的应用前景。

**Para2 Token caching是什么+现有方法怎么做**

Token caching的核心思想是：在多步推理中，相邻帧间大量视觉内容不变却被重复计算，可通过识别静态token并跨帧复用其已计算的表征来节省开销。现有方法主要采用三步策略——(1)通过patch余弦相似度识别静态token；(2)利用cross-attention分数过滤任务相关token防止误缓存；(3)基于注意力熵的层自适应复用比例调整。在**相机固定、任务单一**的场景中，这一范式已取得显著的加速效果且几乎无精度损失。

**Para3 喷SOTA——现有caching的根本缺陷是"静态/固定"**

然而，现有token caching方法本质上是**静态**的——它依赖两个隐含假设：(1)相机不动，因此同一位置的patch可以直接逐帧对比来判定是否变化；(2)任务关注的区域在整个推理过程中固定不变，一旦标记即可全程复用。当这两个假设被打破时，现有方法将不再适用。

**Para4 Challenge——VLN天然具有两种动态性，打破了上述静态假设**

VLN场景天然具有两种动态性，恰好打破了上述两个假设：(1)视觉动态性(Visual Dynamics)——相机随智能体持续运动，同一背景在相邻帧间因视角变换而呈现不同外观，逐位置的patch对比完全失效，即"看到的东西在变"；(2)语义动态性(Semantic Dynamics)——导航指令涉及一系列空间地标（如"经过沙发后左转"），随导航进程推进，当前关键地标不断切换，哪些token语义关键是动态变化的，固定的标记无法应对，即"关注的东西在变"。

**Para5 分析→insight**

针对上述两种动态性，我们进行了实证分析。对于视觉动态性，我们统计了VLN导航中相邻帧的patch相似度分布，发现视角运动导致大量实际未变的背景区域相似度大幅下降，被错判为"已变化"——这说明视角变化是造成cache失效的直接原因，若能补偿视角差异，静态token仍可被正确识别。对于语义动态性，我们分析了导航过程中地标token的语义关注度变化，发现地标token(如"沙发")虽然视觉外观几乎不变，但其语义重要性在导航进程中显著波动（接近时高、经过后骤降）——这说明"视觉静态≠可安全缓存"，必须感知语义重要性的变化才能决定是否重算。

**Para6 方法概述**

基于上述分析，我们提出VLN-Cache——一个同时具备视觉变化感知(Visual Awareness)和语义变化感知(Semantic Awareness)的动态token缓存框架：
针对视觉动态性，提出视角变化感知的token缓存判别，识别因视角变化而表面不同但实际对应相同场景内容的token，将其正确纳入缓存复用；
针对语义动态性，提出指令感知的语义重要性变化检测，感知哪些token的语义重要性随导航进程发生了变化，将其从缓存中召回并触发动态重算；
此外，引入层自适应复用策略，根据各层注意力熵动态调整缓存比例，平衡效率与精度。

Para7 贡献：
- 揭示了现有token caching方法的"静态"本质局限，指出VLN场景中存在视觉动态性和语义动态性两个核心挑战；
- 提出Visual Awareness：视角感知的token缓存判别，将视角变化但内容不变的token正确纳入缓存；
- 提出Semantic Awareness：指令感知的语义重要性变化检测，将语义重要性发生变化的token从缓存中召回并动态重算；

实验验证加速xx×且导航性能几乎不降。



### **2. Preliminary**

#### **2.1 Vision-Language Navigation**

VLN任务定义：智能体在未见过的3D环境中，根据自然语言指令进行导航。每一步 $t$，智能体获得视觉观测 $o_t$，结合语言指令 $L$ 和历史信息 $H_t$，由VLN模型生成导航动作 $a_t$：
$$a_t = \arg\max_{a} P(a \mid o_t, L, H_t; \theta)$$
VLN模型通常包含三个组件：视觉编码器（将图像 $o_t$ 编码为视觉token序列 $\{v_1, ..., v_M\}$）、语言主干（融合多模态信息并进行推理）、动作生成头（将输出token映射为导航动作）。每个动作token $a_t$ 基于之前的token $a_{0:t-1}$、视觉观测 $o_t$、语言指令 $L$ 自回归生成。

#### **2.2 Token Caching**

在Transformer自回归推理中，标准KV Cache通过缓存已生成token的 $k, v$ 避免单次推理内的重复计算，但无法跨帧复用——每一步新观测到来时，所有视觉token的KV必须从头计算。Cross-frame token caching将这一思想扩展到帧间：对于相邻帧中内容未变的视觉token，直接复用其上一帧已计算的KV表征，跳过重复前向计算。其核心流程可形式化为：
$$\hat{K}_t^{(i)} = \begin{cases} K_{t-1}^{(i)}, & \text{if} \ \cos(p_t^{(i)}, p_{t-1}^{(i)}) > \tau \\ f_K(v_t^{(i)}), & \text{otherwise} \end{cases}$$
其中 $p_t^{(i)}$ 为第 $i$ 个patch的特征，$\tau$ 为静态判定阈值。相似度高于阈值的token被判定为"静态"，直接复用缓存；否则重新计算。在此基础上，还可通过语言-视觉交叉注意力分数保护任务相关token、通过注意力熵实现层自适应复用比例。该范式已在相机固定的操控场景中验证有效（如VLA-Cache），实现显著加速且精度几乎无损。

### **3. Observation and Motivation**

本节首先将现有token caching直接应用于VLN场景并测试其性能，随后分别从视觉动态性和语义动态性两个维度分析失败原因，给出定量证据并推导出针对性的设计洞察。

#### **3.1 Analysis for Visual Dynamics**

**实验设置**：在R2R-CE导航数据集上，将Preliminary中介绍的跨帧token caching范式直接应用于VLN模型。统计相邻帧间各patch的余弦相似度分布，并与相机固定的操控场景进行对比；同时在不同阈值设置下测试token caching的加速效果与导航性能。

**预期结果(Fig./Tab.)**：
- 操控场景中，相邻帧patch相似度普遍很高，静态判定准确率高；
- VLN场景中，由于智能体运动带来的视角变化，即使是实际未变的背景区域，相似度也大幅下降，大量token被错判为"已变化"而被迫重新计算；
- 直接应用token caching后，要么缓存率极低（几乎没有加速），要么阈值放松后将真正变化的token误判为"静态"导致导航性能下降。

**Insight**：视角变化是造成逐位置patch对比失效的直接原因，而非场景内容本身发生了变化。若能在对比前补偿帧间视角差异，被视角干扰的静态token仍可被正确识别和复用。

#### **3.2 Analysis for Semantic Dynamics**

**实验设置**：在3.1的基础上，假设视觉动态性已被解决（即静态token已能被正确识别）。选取包含多个地标的导航指令（如"Walk past the sofa, turn left at the table"），提取VLN模型各步的语言-视觉交叉注意力分数，追踪特定地标token（如"sofa"对应的视觉区域）在整条轨迹上的语义关注度变化。

**预期结果(Fig./Tab.)**：

- 地标token的视觉相似度较高，被正确判定为"静态"并持续缓存；
- 但其**语义关注度**（cross-attention分数）呈现剧烈波动：智能体接近沙发时关注度快速上升，经过后骤降，同时"table"的关注度开始攀升；
- 一旦地标token在语义关键时刻被缓存复用（而非重新计算），缓存中保存的是旧的计算结果，无法反映该token当前语义重要性的变化，导致导航决策偏差，SR显著下降。

**Insight**：即使解决了视觉动态性，仍存在更深层的问题——视觉静态 ≠ 可安全缓存。导航指令中地标的语义重要性随进程动态切换，现有的一次性语义标记无法捕捉这种变化。必须引入指令感知的动态语义检测，在token的语义重要性发生显著变化时，将其从缓存复用集中移除并强制重新计算。



### 4. VLN-Cache: Token Caching Framework

基于Chapter 3的两个洞察，本章提出VLN-Cache框架。4.1和4.2对应Visual Awareness，解决视觉动态性；4.3对应Semantic Awareness，解决语义动态性；4.4为层自适应复用。

#### 4.1 View-Aware Feature Alignment

承接3.1 insight：视角变化导致patch对比失效。通过几何变换将前后帧特征对齐到同一视角坐标系下，恢复静态token的可判别性。

#### 4.2 Cross-Frame Token Reusing

对齐后判定为静态的token直接复用缓存，跳过前向计算。给出完整单步缓存流程和算法伪代码。

#### 4.3 Instruction-Guided Saliency Filtering

承接3.2 insight：视觉静态≠可安全缓存。每步动态检测各token语义关注度的**变化量**，发生显著跃变时从缓存中移除并强制重算。与现有方法的区别：检测变化量而非一次性标记。

#### 4.4 Layer-Adaptive Token Reusing

基于各层注意力熵动态调整逐层复用比例，浅层保守、深层激进。

### **5. Experiments**

#### **5.1 Setup**

数据集：R2R-CE / RxR-CE / Social-VLN。基线：原始VLN全量推理、VLA-Cache naive迁移、其他VLN加速方法（Walk and Read Less, Harnessing Input-Adaptive Inference）。指标：SR, SPL, NE, Speedup。

#### **5.2 Main Results**

主表：三个数据集上对比所有方法的导航性能与加速效果。

#### **5.3 Ablation Study**

逐模块消融（w/o 4.1 / w/o 4.3 / w/o 4.4 / w/o 4.1+4.3退化为naive）。

#### **5.4 Discussion**

Overhead分析、超参数敏感性（$\tau$, $\delta$）、适用性讨论。

### **6. Conclusion**

总结贡献 + 未来工作。

